# the following config worked, this assumes that
# a VPC is created, security groups are set, DHCP is set, instances launched in the VPC can communicate from outside 
# cfncluster configure -c ~/.cfncluster/config_vpc vpchpc
# cfncluster create --norollback -c ~/.cfncluster/config_vpc vpchpc

[global]
cluster_template = default
update_check = true
sanity_check = true

[aws]
aws_access_key_id = AxxxQ
aws_secret_access_key = sxxxj
aws_region_name = us-east-1

[cluster default]
key_name = sshkey
compute_instance_type = t2.micro
master_instance_type = t2.micro
initial_queue_size = 1
max_queue_size = 10              # check or request to increase max number allowed from EC2 Service Limits
maintain_initial_size = false    # work with --norollback for trouble shooting
scheduler = sge
cluster_type = spot
spot_price = 1.00
ephemeral_dir = /scratch
shared_dir = /shared
encrypted_ephemeral = false
master_root_volume_size = 15     # GB
compute_root_volume_size = 15
base_os = alinux
post_install = s3://xxx/cfncluster_adduser.sh # script to add users to compute node, must be public readable
vpc_settings = elmahpc
ebs_settings = custom
scaling_settings = custom

[vpc elmahpc]
ssh_from = 0.0.0.0/0             # still needs proper security group to open up
use_public_ips = true            # if set to false, need to set up NAT Gateway in VPC, refer to https://github.com/awslabs/cfncluster/issues/531
vpc_id = vpc-xxx                 # make sure this vpc works, interfaces, gateways, DCHP, security groups are correctly setup
master_subnet_id = subnet-xxx    # such as 10.0.0.128/26, to define a block of IPs
vpc_security_group_id = sg-xxx   # make sure within VPC all are open, SSH should be open to public or VPN
additional_sg = sg-xxx           # just more customer rules[ebs custom]

[ebs custom]
volume_type = gp2
volume_iops = 100
volume_size = 50

[scaling custom]
scaledown_idletime = 50



[aliases]
ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}
